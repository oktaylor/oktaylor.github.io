---
---
@misc{arxiv25,
abbr={ARXIV}
title={Event-based Facial Keypoint Alignment via Cross-Modal Fusion Attention and Self-Supervised Multi-Event Representation Learning},
author={Donghwa Kang and Junho Kim and Dongwoo Kang},
year={2025},
eprint={2509.24968},
archivePrefix={arXiv},
primaryClass={cs.CV},
arxiv={https://arxiv.org/abs/2509.24968},
abstract={Event cameras offer unique advantages for facial keypoint alignment under challenging conditions, such as low light and rapid motion, due to their high temporal resolution and robustness to varying illumination. However, existing RGB facial keypoint alignment methods do not perform well on event data, and training solely on event data often leads to suboptimal performance because of its limited spatial information. Moreover, the lack of comprehensive labeled event datasets further hinders progress in this area. To address these issues, we propose a novel framework based on cross-modal fusion attention (CMFA) and self-supervised multi-event representation learning (SSMER) for event-based facial keypoint alignment. Our framework employs CMFA to integrate corresponding RGB data, guiding the model to extract robust facial features from event input images. In parallel, SSMER enables effective feature learning from unlabeled event data, overcoming spatial limitations. Extensive experiments on our real-event E-SIE dataset and a synthetic-event version of the public WFLW-V benchmark show that our approach consistently surpasses state-of-the-art methods across multiple evaluation metrics.}
}

@ARTICLE{access24,
abbr={ACCESS}
author={Kang, Donghwa and Kang, Dongwoo},
journal={IEEE Access},
title={Head Pose-Aware Regression for Pupil Localization From a-Pillar Cameras},
year={2024},
volume={12},
number={},
pages={11083-11094},
keywords={Pupils;Location awareness;Magnetic heads;Cameras;Transformers;Pose estimation;Eyes;Augmented reality;Gaze tracking;Pupil center localization;remote eye tracking;head pose-aware pupil regression;eye-nose points regression;head pose estimation;A-pillar camera;augmented reality (AR) 3D head-up displays (HUDs);driver monitoring system (DMS)},
doi={https://doi.org/10.1109/ACCESS.2024.3354373},
abstract={In vehicular applications, remote eye pupil tracking is essential, particularly for advanced augmented reality (AR) 3D head-up displays (HUDs), and driver monitoring systems (DMS). However, achieving accurate pupil center localization under varied head poses presents significant challenges, especially when cameras are positioned on a vehicle’s A-pillar. This placement introduces substantial head pose variations, complicating traditional tracking methods. In response, this study presents a remote pupil localization method designed to address the unique challenges posed by a camera situated on a vehicle’s A-pillar, a spot causing significant head pose variations. The proposed technique relies on a head pose-aware pupil localization strategy utilizing A-pillar cameras. Our pupil localization algorithm adopts a Transformer regression approach, into which we integrate head pose estimation data, enhancing its capability across diverse head poses. To further enhance our approach, we used an optimized nine-point eye-nose landmark set, to minimize the pupil center localization loss. To demonstrate the robustness of our method, we conducted evaluations using both the public WIDER Facial Landmarks in-the-wild (WFLW) dataset and a custom in-house dataset focused on A-pillar camera captures. Results indicate a Normalized Mean Error (NME) of 2.79% and a failure rate (FR) of 1.28% on the WFLW dataset. On our in-house dataset, the method achieved an NME of 2.96% and a FR of 0.72%. These impressive results demonstrate the robustness and efficacy of our method, suggesting its potential for implementation in commercial eye tracking systems using A-pillar mounted cameras, especially for AR 3D HUD and DMS applications.}
}

@article{eswa25,
abbr = {ESWA},
title = {An adaptive learning framework for event-based remote eye tracking},
journal = {Expert Systems with Applications},
volume = {286},
pages = {128038},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.128038},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425016598},
author = {Donghwa Kang and Dongwoo Kang},
keywords = {Event camera, Event-based remote eye tracking, Feature tracking, Synthetic event generation, Application of artificial intelligence (AI)},
abstract = {Event cameras are next-generation asynchronous image sensors that detect only changes in light intensity. Because event cameras can capture fast-moving objects without motion blur, they have gained attention as a suitable technology for tracking the human eye, the fastest-moving part of the body. While there has been significant progress in near-range eye tracking, event-based remote eye tracking is still in its early stages due to the challenge of limited spatial information, resulting in fewer studies in this area. In this paper, we propose a novel framework for remote eye tracking using event cameras, incorporating the application of artificial intelligence (AI). Our framework addresses the challenge of limited remote eye tracking datasets by transforming frame video into event streams and generating eye annotations. We also select optimal eye keypoints suitable for event-based tracking and predict their displacement using an event-based feature tracking network. The method detects initial keypoints from a single frame, generates a reference patch, and combines it with event patches for event feature tracking. To validate our model, we conducted comprehensive evaluations on a self-collected dataset, which includes various face angles and lighting conditions, including low light environments. Our proposed method achieves a feature age of 0.550 and an expected feature age of 0.549, demonstrating promising results for event-based remote eye tracking.}
}
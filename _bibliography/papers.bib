---
---

@string{aps = {American Physical Society,}}

@misc{kang2025eventbasedfacialkeypointalignment,
      title={Event-based Facial Keypoint Alignment via Cross-Modal Fusion Attention and Self-Supervised Multi-Event Representation Learning},
      author={Donghwa Kang and Junho Kim and Dongwoo Kang},
      year={2025},
      eprint={2509.24968},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2509.24968},
}

@ARTICLE{10400480,
  author={Kang, Donghwa and Kang, Dongwoo},
  journal={IEEE Access},
  title={Head Pose-Aware Regression for Pupil Localization From a-Pillar Cameras},
  year={2024},
  volume={12},
  number={},
  pages={11083-11094},
  keywords={Pupils;Location awareness;Magnetic heads;Cameras;Transformers;Pose estimation;Eyes;Augmented reality;Gaze tracking;Pupil center localization;remote eye tracking;head pose-aware pupil regression;eye-nose points regression;head pose estimation;A-pillar camera;augmented reality (AR) 3D head-up displays (HUDs);driver monitoring system (DMS)},
  doi={10.1109/ACCESS.2024.3354373}}

@article{KANG2025128038,
title = {An adaptive learning framework for event-based remote eye tracking},
journal = {Expert Systems with Applications},
volume = {286},
pages = {128038},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.128038},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425016598},
author = {Donghwa Kang and Dongwoo Kang},
keywords = {Event camera, Event-based remote eye tracking, Feature tracking, Synthetic event generation, Application of artificial intelligence (AI)},
abstract = {Event cameras are next-generation asynchronous image sensors that detect only changes in light intensity. Because event cameras can capture fast-moving objects without motion blur, they have gained attention as a suitable technology for tracking the human eye, the fastest-moving part of the body. While there has been significant progress in near-range eye tracking, event-based remote eye tracking is still in its early stages due to the challenge of limited spatial information, resulting in fewer studies in this area. In this paper, we propose a novel framework for remote eye tracking using event cameras, incorporating the application of artificial intelligence (AI). Our framework addresses the challenge of limited remote eye tracking datasets by transforming frame video into event streams and generating eye annotations. We also select optimal eye keypoints suitable for event-based tracking and predict their displacement using an event-based feature tracking network. The method detects initial keypoints from a single frame, generates a reference patch, and combines it with event patches for event feature tracking. To validate our model, we conducted comprehensive evaluations on a self-collected dataset, which includes various face angles and lighting conditions, including low light environments. Our proposed method achieves a feature age of 0.550 and an expected feature age of 0.549, demonstrating promising results for event-based remote eye tracking.}
}